{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wefQoVdqVTED"
      },
      "source": [
        "# Selecció del repositori"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFpn1yYAufxj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "d96a7c9d-f6ba-491c-bfc0-73fdbbbaeec4"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir('/content/drive/My Drive/Model_Training')\n",
        "\n",
        "!pip install detecto"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Collecting detecto\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/fe/2a6b29c30189051b0f0308fb6f1a1a386069eaa200b89f985ef1f3a9dc85/detecto-1.1.3-py3-none-any.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from detecto) (1.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from detecto) (0.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from detecto) (3.2.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from detecto) (4.1.2.30)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from detecto) (0.16.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from detecto) (1.0.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->detecto) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->detecto) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision->detecto) (1.18.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detecto) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detecto) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detecto) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detecto) (1.1.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->detecto) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->detecto) (2.4)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->detecto) (2.4.1)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->detecto) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->detecto) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->detecto) (46.0.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->detecto) (4.4.2)\n",
            "Installing collected packages: detecto\n",
            "Successfully installed detecto-1.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2YJpuEwvGMF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3822f96d-1988-4d87-b0f6-5c03abba0429"
      },
      "source": [
        "# Comprobam si esteim en el directori correcte\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frozen_inference_graph.pb  pipeline.config  train_labels      val_labels\n",
            "models\t\t\t   resnet_101.pth   train_labels.csv  val_labels.csv\n",
            "Model_training.ipynb\t   train_images     val_images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfgQhTu8TupD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "abda7874-483d-4521-c3b3-27f6655e70df"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.tools.graph_transforms import TransformGraph\n",
        "from tensorflow.core.framework import graph_pb2\n",
        "import copy\n",
        "\n",
        "def load_graph(filename):\n",
        "    graph_def = tf.GraphDef()\n",
        "    with tf.gfile.FastGFile(filename, 'rb') as f:\n",
        "        graph_def.ParseFromString(f.read())\n",
        "    return graph_def\n",
        "\n",
        "\n",
        "def transform_graph(input_graph, output_graph=None):\n",
        "    \"\"\" Use to run different transform function on the input graph and generate a output graph. \"\"\"\n",
        "    if isinstance(input_graph, graph_pb2.GraphDef):\n",
        "        graph_def = input_graph\n",
        "    else:\n",
        "        graph_def = load_graph(input_graph)\n",
        "        \n",
        "    new_graph_def = TransformGraph(graph_def, ['input_placeholder/input_image'], ['predicated_output'], \n",
        "            ['strip_unused_nodes(type=float, shape=\"1,28,28,1\")', 'remove_nodes(op=Identity, op=CheckNumerics, op=Switch)', \n",
        "            'fold_constants(ignore_errors=true)', 'fold_batch_norms', 'fold_old_batch_norms', 'sort_by_execution_order'])\n",
        "\n",
        "    if output_graph == None:\n",
        "        return new_graph_def\n",
        "\n",
        "    # save new graph\n",
        "    with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
        "        f.write(new_graph_def.SerializeToString())\n",
        "    \n",
        "    # tf.io.write_graph(od_graph_def, \"\", output_graph, as_text=False)\n",
        "\n",
        "\n",
        "def convert_to_constant(input_graph, output_graph=None):\n",
        "    \"\"\" Convert the placeholders in graph to constant nodes. \"\"\"\n",
        "    if isinstance(input_graph, graph_pb2.GraphDef):\n",
        "        graph_def = input_graph\n",
        "    else:\n",
        "        graph_def = load_graph(input_graph)\n",
        "\n",
        "    keep_prob = tf.constant(1.0, dtype=tf.float32, shape=[], name='keep_prob')\n",
        "    weight_factor = tf.constant(1.0, dtype=tf.float32, shape=[], name='weight_factor')\n",
        "    is_training = tf.constant(False, dtype=tf.bool, shape=[], name='is_training')\n",
        "\n",
        "    new_graph_def = graph_pb2.GraphDef()\n",
        "\n",
        "    for node in graph_def.node:\n",
        "        if node.name == 'keep_prob':\n",
        "            new_graph_def.node.extend([keep_prob.op.node_def])\n",
        "\n",
        "        elif node.name == 'weight_factor':\n",
        "            new_graph_def.node.extend([weight_factor.op.node_def])\n",
        "\n",
        "        elif node.name == 'is_training':\n",
        "            new_graph_def.node.extend([is_training.op.node_def])\n",
        "\n",
        "        else:\n",
        "            new_graph_def.node.extend([copy.deepcopy(node)])\n",
        "\n",
        "    \n",
        "    if output_graph == None:\n",
        "        return new_graph_def\n",
        "\n",
        "    # save new graph\n",
        "    with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
        "        f.write(new_graph_def.SerializeToString())\n",
        "\n",
        "\n",
        "def optimize_batch_normalization(input_graph, output_graph=None):\n",
        "    \"\"\" Optimize the batch normalization block. \"\"\"\n",
        "    if isinstance(input_graph, graph_pb2.GraphDef):\n",
        "        graph_def = input_graph\n",
        "    else:\n",
        "        graph_def = load_graph(input_graph)\n",
        "\n",
        "    new_graph_def = graph_pb2.GraphDef()\n",
        "    unused_attrs = ['is_training']              # Attributes of FusedBatchNorm. Not needed during inference.\n",
        "\n",
        "    # All the node names are specific to my ocr model.\n",
        "    # All the input names are found manually from tensorboard\n",
        "    for node in graph_def.node:\n",
        "        modified_node = copy.deepcopy(node)\n",
        "        if node.name.startswith(\"conv\"):        # True for Convolutional Layers\n",
        "            starting_name = \"\"\n",
        "            if node.name.startswith(\"conv1\"):\n",
        "                starting_name = \"conv1\"\n",
        "\n",
        "            elif node.name.startswith(\"conv2\"):\n",
        "                starting_name = \"conv2\"\n",
        "\n",
        "            elif node.name.startswith(\"conv3\"):\n",
        "                starting_name = \"conv3\"\n",
        "\n",
        "            elif node.name.startswith(\"conv4\"):\n",
        "                starting_name = \"conv4\"\n",
        "\n",
        "            # Do not add the cond block and its child nodes. \n",
        "            # This is only needed during training.\n",
        "            if \"cond\" in node.name and not node.name.endswith(\"FusedBatchNorm\"):\n",
        "                continue\n",
        "\n",
        "            if node.op == \"FusedBatchNorm\" and node.name.endswith(\"FusedBatchNorm\"):\n",
        "                if bool(starting_name):\n",
        "                    # Changing the name to remove one block hierarchy and changing inputs.\n",
        "                    modified_node.name = \"{0}/{0}/batch_norm/FusedBatchNorm\".format(starting_name) \n",
        "                    modified_node.input[0] = \"{}/Conv2D\".format(starting_name)\n",
        "                    modified_node.input[1] = \"{}/batch_norm/gamma\".format(starting_name)\n",
        "                    modified_node.input[2] = \"{}/batch_norm/beta\".format(starting_name)\n",
        "                    modified_node.input[3] = \"{}/batch_norm/moving_mean\".format(starting_name)\n",
        "                    modified_node.input[4] = \"{}/batch_norm/moving_variance\".format(starting_name)\n",
        "\n",
        "                    # Deleting unused attributes\n",
        "                    for attr in unused_attrs:\n",
        "                        if attr in modified_node.attr:\n",
        "                            del modified_node.attr[attr]\n",
        "\n",
        "            if node.name.endswith('activation'):\n",
        "                if bool(starting_name):\n",
        "                    modified_node.input[0] = \"{0}/{0}/batch_norm/FusedBatchNorm\".format(starting_name)\n",
        "\n",
        "        elif node.name.startswith(\"fc\") or node.name.startswith(\"logits\"):  # True for fully connected layers\n",
        "            starting_name = \"\"\n",
        "            if node.name.startswith(\"fc1\"):\n",
        "                starting_name = \"fc1\"\n",
        "\n",
        "            elif node.name.startswith(\"fc2\"):\n",
        "                starting_name = \"fc2\"\n",
        "\n",
        "            elif node.name.startswith(\"logits\"):\n",
        "                starting_name = \"logits\"\n",
        "\n",
        "            # Do not add cond, cond_1 and moments block of batch normalization\n",
        "            if \"cond\" in node.name or \"moments\" in node.name:\n",
        "                continue\n",
        "\n",
        "            # Change input of batchnorm/add\n",
        "            if node.name.endswith('batchnorm/add'):\n",
        "                modified_node.input[0] = \"{}/batch_norm/moving_variance\".format(starting_name)\n",
        "                modified_node.input[1] = \"{0}/{0}/batch_norm/batchnorm/add/y\".format(starting_name)\n",
        "\n",
        "            if node.name.endswith('batchnorm/mul_2'):\n",
        "                modified_node.input[0] = \"{0}/{0}/batch_norm/batchnorm/mul\".format(starting_name)\n",
        "                modified_node.input[1] = \"{}/batch_norm/moving_mean\".format(starting_name)\n",
        "\n",
        "\n",
        "        new_graph_def.node.extend([modified_node])\n",
        "\n",
        "    \n",
        "    if output_graph == None:\n",
        "        return new_graph_def\n",
        "\n",
        "    # save the graph\n",
        "    with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
        "        f.write(new_graph_def.SerializeToString())\n",
        "\n",
        "\n",
        "def remove_dropout(input_graph, output_graph=None):\n",
        "    \"\"\" Remove the dropout block from the model. \"\"\"\n",
        "    if isinstance(input_graph, graph_pb2.GraphDef):\n",
        "        graph_def = input_graph\n",
        "    else:\n",
        "        graph_def = load_graph(input_graph)\n",
        "\n",
        "    new_graph_def = graph_pb2.GraphDef()\n",
        "\n",
        "    for node in graph_def.node:\n",
        "        modified_node = copy.deepcopy(node)\n",
        "        if node.name.startswith('dropout1') or node.name.startswith('dropout2'):\n",
        "            continue\n",
        "\n",
        "        if node.name == \"fc2/fc2/batch_norm/batchnorm/mul_1\":\n",
        "            modified_node.input[0] = \"mul\"\n",
        "            modified_node.input[1] = \"fc2/weights\"\n",
        "\n",
        "        if node.name == \"logits/logits/batch_norm/batchnorm/mul_1\":\n",
        "            modified_node.input[0] = \"fc2/activation\"\n",
        "            modified_node.input[1] = \"logits/weights\"\n",
        "\n",
        "        new_graph_def.node.extend([modified_node])\n",
        "\n",
        "    \n",
        "    if output_graph == None:\n",
        "        return new_graph_def\n",
        "\n",
        "    # save the graph\n",
        "    with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
        "        f.write(new_graph_def.SerializeToString())\n",
        "        \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # frozen_graph is the output of freeze_graph.py file\n",
        "    frozen_graph = \"frozen_inference_graph.pb\"\n",
        "    # Final graph file to be use with opencv dnn module\n",
        "    output_graph = \"frozen_inference_graph_dnn.pb\"\n",
        "    graph_def = transform_graph(frozen_graph)\n",
        "    graph_def = convert_to_constant(graph_def)\n",
        "    graph_def = optimize_batch_normalization(graph_def)\n",
        "    graph_def = transform_graph(graph_def)\n",
        "    remove_dropout(graph_def, output_graph)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-77b0ccce0a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgraph_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.tools.graph_transforms'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGi7z1WPeL_N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "c89f5882-b099-428f-9446-0c457a29e11c"
      },
      "source": [
        "import argparse\n",
        "import re\n",
        "import six\n",
        "from math import sqrt\n",
        "from cv2 import tf_text_graph_common\n",
        "\n",
        "class SSDAnchorGenerator:\n",
        "    def __init__(self, min_scale, max_scale, num_layers, aspect_ratios,\n",
        "                 reduce_boxes_in_lowest_layer, image_width, image_height):\n",
        "        self.min_scale = min_scale\n",
        "        self.aspect_ratios = aspect_ratios\n",
        "        self.reduce_boxes_in_lowest_layer = reduce_boxes_in_lowest_layer\n",
        "        self.image_width = image_width\n",
        "        self.image_height = image_height\n",
        "        self.scales =  [min_scale + (max_scale - min_scale) * i / (num_layers - 1)\n",
        "                            for i in range(num_layers)] + [1.0]\n",
        "\n",
        "    def get(self, layer_id):\n",
        "        if layer_id == 0 and self.reduce_boxes_in_lowest_layer:\n",
        "            widths = [0.1, self.min_scale * sqrt(2.0), self.min_scale * sqrt(0.5)]\n",
        "            heights = [0.1, self.min_scale / sqrt(2.0), self.min_scale / sqrt(0.5)]\n",
        "        else:\n",
        "            widths = [self.scales[layer_id] * sqrt(ar) for ar in self.aspect_ratios]\n",
        "            heights = [self.scales[layer_id] / sqrt(ar) for ar in self.aspect_ratios]\n",
        "\n",
        "            widths += [sqrt(self.scales[layer_id] * self.scales[layer_id + 1])]\n",
        "            heights += [sqrt(self.scales[layer_id] * self.scales[layer_id + 1])]\n",
        "        min_size = min(self.image_width, self.image_height)\n",
        "        widths = [w * min_size for w in widths]\n",
        "        heights = [h * min_size for h in heights]\n",
        "        return widths, heights\n",
        "\n",
        "\n",
        "class MultiscaleAnchorGenerator:\n",
        "    def __init__(self, min_level, aspect_ratios, scales_per_octave, anchor_scale):\n",
        "        self.min_level = min_level\n",
        "        self.aspect_ratios = aspect_ratios\n",
        "        self.anchor_scale = anchor_scale\n",
        "        self.scales = [2**(float(s) / scales_per_octave) for s in range(scales_per_octave)]\n",
        "\n",
        "    def get(self, layer_id):\n",
        "        widths = []\n",
        "        heights = []\n",
        "        for a in self.aspect_ratios:\n",
        "            for s in self.scales:\n",
        "                base_anchor_size = 2**(self.min_level + layer_id) * self.anchor_scale\n",
        "                ar = sqrt(a)\n",
        "                heights.append(base_anchor_size * s / ar)\n",
        "                widths.append(base_anchor_size * s * ar)\n",
        "        return widths, heights\n",
        "\n",
        "\n",
        "def createSSDGraph(modelPath, configPath, outputPath):\n",
        "    # Nodes that should be kept.\n",
        "    keepOps = ['Conv2D', 'BiasAdd', 'Add', 'AddV2', 'Relu', 'Relu6', 'Placeholder', 'FusedBatchNorm',\n",
        "               'DepthwiseConv2dNative', 'ConcatV2', 'Mul', 'MaxPool', 'AvgPool', 'Identity',\n",
        "               'Sub', 'ResizeNearestNeighbor', 'Pad', 'FusedBatchNormV3', 'Mean']\n",
        "\n",
        "    # Node with which prefixes should be removed\n",
        "    prefixesToRemove = ('MultipleGridAnchorGenerator/', 'Concatenate/', 'Postprocessor/', 'Preprocessor/map')\n",
        "\n",
        "    # Load a config file.\n",
        "    config = readTextMessage(configPath)\n",
        "    config = config['model'][0]['ssd'][0]\n",
        "    num_classes = int(config['num_classes'][0])\n",
        "\n",
        "    fixed_shape_resizer = config['image_resizer'][0]['fixed_shape_resizer'][0]\n",
        "    image_width = int(fixed_shape_resizer['width'][0])\n",
        "    image_height = int(fixed_shape_resizer['height'][0])\n",
        "\n",
        "    box_predictor = 'convolutional' if 'convolutional_box_predictor' in config['box_predictor'][0] else 'weight_shared_convolutional'\n",
        "\n",
        "    anchor_generator = config['anchor_generator'][0]\n",
        "    if 'ssd_anchor_generator' in anchor_generator:\n",
        "        ssd_anchor_generator = anchor_generator['ssd_anchor_generator'][0]\n",
        "        min_scale = float(ssd_anchor_generator['min_scale'][0])\n",
        "        max_scale = float(ssd_anchor_generator['max_scale'][0])\n",
        "        num_layers = int(ssd_anchor_generator['num_layers'][0])\n",
        "        aspect_ratios = [float(ar) for ar in ssd_anchor_generator['aspect_ratios']]\n",
        "        reduce_boxes_in_lowest_layer = True\n",
        "        if 'reduce_boxes_in_lowest_layer' in ssd_anchor_generator:\n",
        "            reduce_boxes_in_lowest_layer = ssd_anchor_generator['reduce_boxes_in_lowest_layer'][0] == 'true'\n",
        "        priors_generator = SSDAnchorGenerator(min_scale, max_scale, num_layers,\n",
        "                                              aspect_ratios, reduce_boxes_in_lowest_layer,\n",
        "                                              image_width, image_height)\n",
        "\n",
        "\n",
        "        print('Scale: [%f-%f]' % (min_scale, max_scale))\n",
        "        print('Aspect ratios: %s' % str(aspect_ratios))\n",
        "        print('Reduce boxes in the lowest layer: %s' % str(reduce_boxes_in_lowest_layer))\n",
        "    elif 'multiscale_anchor_generator' in anchor_generator:\n",
        "        multiscale_anchor_generator = anchor_generator['multiscale_anchor_generator'][0]\n",
        "        min_level = int(multiscale_anchor_generator['min_level'][0])\n",
        "        max_level = int(multiscale_anchor_generator['max_level'][0])\n",
        "        anchor_scale = float(multiscale_anchor_generator['anchor_scale'][0])\n",
        "        aspect_ratios = [float(ar) for ar in multiscale_anchor_generator['aspect_ratios']]\n",
        "        scales_per_octave = int(multiscale_anchor_generator['scales_per_octave'][0])\n",
        "        num_layers = max_level - min_level + 1\n",
        "        priors_generator = MultiscaleAnchorGenerator(min_level, aspect_ratios,\n",
        "                                                     scales_per_octave, anchor_scale)\n",
        "        print('Levels: [%d-%d]' % (min_level, max_level))\n",
        "        print('Anchor scale: %f' % anchor_scale)\n",
        "        print('Scales per octave: %d' % scales_per_octave)\n",
        "        print('Aspect ratios: %s' % str(aspect_ratios))\n",
        "    else:\n",
        "        print('Unknown anchor_generator')\n",
        "        exit(0)\n",
        "\n",
        "    print('Number of classes: %d' % num_classes)\n",
        "    print('Number of layers: %d' % num_layers)\n",
        "    print('box predictor: %s' % box_predictor)\n",
        "    print('Input image size: %dx%d' % (image_width, image_height))\n",
        "\n",
        "    # Read the graph.\n",
        "    _inpNames = ['image_tensor']\n",
        "    outNames = ['num_detections', 'detection_scores', 'detection_boxes', 'detection_classes']\n",
        "\n",
        "    writeTextGraph(modelPath, outputPath, outNames)\n",
        "    graph_def = parseTextGraph(outputPath)\n",
        "\n",
        "    def getUnconnectedNodes():\n",
        "        unconnected = []\n",
        "        for node in graph_def.node:\n",
        "            unconnected.append(node.name)\n",
        "            for inp in node.input:\n",
        "                if inp in unconnected:\n",
        "                    unconnected.remove(inp)\n",
        "        return unconnected\n",
        "\n",
        "\n",
        "    def fuse_nodes(nodesToKeep):\n",
        "        # Detect unfused batch normalization nodes and fuse them.\n",
        "        # Add_0 <-- moving_variance, add_y\n",
        "        # Rsqrt <-- Add_0\n",
        "        # Mul_0 <-- Rsqrt, gamma\n",
        "        # Mul_1 <-- input, Mul_0\n",
        "        # Mul_2 <-- moving_mean, Mul_0\n",
        "        # Sub_0 <-- beta, Mul_2\n",
        "        # Add_1 <-- Mul_1, Sub_0\n",
        "        nodesMap = {node.name: node for node in graph_def.node}\n",
        "        subgraphBatchNorm = ['Add',\n",
        "            ['Mul', 'input', ['Mul', ['Rsqrt', ['Add', 'moving_variance', 'add_y']], 'gamma']],\n",
        "            ['Sub', 'beta', ['Mul', 'moving_mean', 'Mul_0']]]\n",
        "        subgraphBatchNormV2 = ['AddV2',\n",
        "            ['Mul', 'input', ['Mul', ['Rsqrt', ['AddV2', 'moving_variance', 'add_y']], 'gamma']],\n",
        "            ['Sub', 'beta', ['Mul', 'moving_mean', 'Mul_0']]]\n",
        "        # Detect unfused nearest neighbor resize.\n",
        "        subgraphResizeNN = ['Reshape',\n",
        "            ['Mul', ['Reshape', 'input', ['Pack', 'shape_1', 'shape_2', 'shape_3', 'shape_4', 'shape_5']],\n",
        "                    'ones'],\n",
        "            ['Pack', ['StridedSlice', ['Shape', 'input'], 'stack', 'stack_1', 'stack_2'],\n",
        "                     'out_height', 'out_width', 'out_channels']]\n",
        "        def checkSubgraph(node, targetNode, inputs, fusedNodes):\n",
        "            op = targetNode[0]\n",
        "            if node.op == op and (len(node.input) >= len(targetNode) - 1):\n",
        "                fusedNodes.append(node)\n",
        "                for i, inpOp in enumerate(targetNode[1:]):\n",
        "                    if isinstance(inpOp, list):\n",
        "                        if not node.input[i] in nodesMap or \\\n",
        "                           not checkSubgraph(nodesMap[node.input[i]], inpOp, inputs, fusedNodes):\n",
        "                            return False\n",
        "                    else:\n",
        "                        inputs[inpOp] = node.input[i]\n",
        "\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "\n",
        "        nodesToRemove = []\n",
        "        for node in graph_def.node:\n",
        "            inputs = {}\n",
        "            fusedNodes = []\n",
        "            if checkSubgraph(node, subgraphBatchNorm, inputs, fusedNodes) or \\\n",
        "               checkSubgraph(node, subgraphBatchNormV2, inputs, fusedNodes):\n",
        "                name = node.name\n",
        "                node.Clear()\n",
        "                node.name = name\n",
        "                node.op = 'FusedBatchNorm'\n",
        "                node.input.append(inputs['input'])\n",
        "                node.input.append(inputs['gamma'])\n",
        "                node.input.append(inputs['beta'])\n",
        "                node.input.append(inputs['moving_mean'])\n",
        "                node.input.append(inputs['moving_variance'])\n",
        "                node.addAttr('epsilon', 0.001)\n",
        "                nodesToRemove += fusedNodes[1:]\n",
        "\n",
        "            inputs = {}\n",
        "            fusedNodes = []\n",
        "            if checkSubgraph(node, subgraphResizeNN, inputs, fusedNodes):\n",
        "                name = node.name\n",
        "                node.Clear()\n",
        "                node.name = name\n",
        "                node.op = 'ResizeNearestNeighbor'\n",
        "                node.input.append(inputs['input'])\n",
        "                node.input.append(name + '/output_shape')\n",
        "\n",
        "                out_height_node = nodesMap[inputs['out_height']]\n",
        "                out_width_node = nodesMap[inputs['out_width']]\n",
        "                out_height = int(out_height_node.attr['value']['tensor'][0]['int_val'][0])\n",
        "                out_width = int(out_width_node.attr['value']['tensor'][0]['int_val'][0])\n",
        "\n",
        "                shapeNode = NodeDef()\n",
        "                shapeNode.name = name + '/output_shape'\n",
        "                shapeNode.op = 'Const'\n",
        "                shapeNode.addAttr('value', [out_height, out_width])\n",
        "                graph_def.node.insert(graph_def.node.index(node), shapeNode)\n",
        "                nodesToKeep.append(shapeNode.name)\n",
        "\n",
        "                nodesToRemove += fusedNodes[1:]\n",
        "        for node in nodesToRemove:\n",
        "            graph_def.node.remove(node)\n",
        "\n",
        "    nodesToKeep = []\n",
        "    fuse_nodes(nodesToKeep)\n",
        "\n",
        "    removeIdentity(graph_def)\n",
        "\n",
        "    def to_remove(name, op):\n",
        "        return (not name in nodesToKeep) and \\\n",
        "               (op == 'Const' or (not op in keepOps) or name.startswith(prefixesToRemove))\n",
        "\n",
        "    removeUnusedNodesAndAttrs(to_remove, graph_def)\n",
        "\n",
        "\n",
        "    # Connect input node to the first layer\n",
        "    assert(graph_def.node[0].op == 'Placeholder')\n",
        "    # assert(graph_def.node[1].op == 'Conv2D')\n",
        "    weights = graph_def.node[1].input[-1]\n",
        "    for i in range(len(graph_def.node[1].input)):\n",
        "        graph_def.node[1].input.pop()\n",
        "    graph_def.node[1].input.append(graph_def.node[0].name)\n",
        "    graph_def.node[1].input.append(weights)\n",
        "\n",
        "    # Create SSD postprocessing head ###############################################\n",
        "\n",
        "    # Concatenate predictions of classes, predictions of bounding boxes and proposals.\n",
        "    def addConcatNode(name, inputs, axisNodeName):\n",
        "        concat = NodeDef()\n",
        "        concat.name = name\n",
        "        concat.op = 'ConcatV2'\n",
        "        for inp in inputs:\n",
        "            concat.input.append(inp)\n",
        "        concat.input.append(axisNodeName)\n",
        "        graph_def.node.extend([concat])\n",
        "\n",
        "    addConstNode('concat/axis_flatten', [-1], graph_def)\n",
        "    addConstNode('PriorBox/concat/axis', [-2], graph_def)\n",
        "\n",
        "    for label in ['ClassPredictor', 'BoxEncodingPredictor' if box_predictor is 'convolutional' else 'BoxPredictor']:\n",
        "        concatInputs = []\n",
        "        for i in range(num_layers):\n",
        "            # Flatten predictions\n",
        "            flatten = NodeDef()\n",
        "            if box_predictor is 'convolutional':\n",
        "                inpName = 'BoxPredictor_%d/%s/BiasAdd' % (i, label)\n",
        "            else:\n",
        "                if i == 0:\n",
        "                    inpName = 'WeightSharedConvolutionalBoxPredictor/%s/BiasAdd' % label\n",
        "                else:\n",
        "                    inpName = 'WeightSharedConvolutionalBoxPredictor_%d/%s/BiasAdd' % (i, label)\n",
        "            flatten.input.append(inpName)\n",
        "            flatten.name = inpName + '/Flatten'\n",
        "            flatten.op = 'Flatten'\n",
        "\n",
        "            concatInputs.append(flatten.name)\n",
        "            graph_def.node.extend([flatten])\n",
        "        addConcatNode('%s/concat' % label, concatInputs, 'concat/axis_flatten')\n",
        "\n",
        "    num_matched_layers = 0\n",
        "    for node in graph_def.node:\n",
        "        if re.match('BoxPredictor_\\d/BoxEncodingPredictor/convolution', node.name) or \\\n",
        "           re.match('BoxPredictor_\\d/BoxEncodingPredictor/Conv2D', node.name) or \\\n",
        "           re.match('WeightSharedConvolutionalBoxPredictor(_\\d)*/BoxPredictor/Conv2D', node.name):\n",
        "            node.addAttr('loc_pred_transposed', True)\n",
        "            num_matched_layers += 1\n",
        "    assert(num_matched_layers == num_layers)\n",
        "\n",
        "    # Add layers that generate anchors (bounding boxes proposals).\n",
        "    priorBoxes = []\n",
        "    boxCoder = config['box_coder'][0]\n",
        "    fasterRcnnBoxCoder = boxCoder['faster_rcnn_box_coder'][0]\n",
        "    boxCoderVariance = [1.0/float(fasterRcnnBoxCoder['x_scale'][0]), 1.0/float(fasterRcnnBoxCoder['y_scale'][0]), 1.0/float(fasterRcnnBoxCoder['width_scale'][0]), 1.0/float(fasterRcnnBoxCoder['height_scale'][0])]\n",
        "    for i in range(num_layers):\n",
        "        priorBox = NodeDef()\n",
        "        priorBox.name = 'PriorBox_%d' % i\n",
        "        priorBox.op = 'PriorBox'\n",
        "        if box_predictor is 'convolutional':\n",
        "            priorBox.input.append('BoxPredictor_%d/BoxEncodingPredictor/BiasAdd' % i)\n",
        "        else:\n",
        "            if i == 0:\n",
        "                priorBox.input.append('WeightSharedConvolutionalBoxPredictor/BoxPredictor/Conv2D')\n",
        "            else:\n",
        "                priorBox.input.append('WeightSharedConvolutionalBoxPredictor_%d/BoxPredictor/BiasAdd' % i)\n",
        "        priorBox.input.append(graph_def.node[0].name)  # image_tensor\n",
        "\n",
        "        priorBox.addAttr('flip', False)\n",
        "        priorBox.addAttr('clip', False)\n",
        "\n",
        "        widths, heights = priors_generator.get(i)\n",
        "\n",
        "        priorBox.addAttr('width', widths)\n",
        "        priorBox.addAttr('height', heights)\n",
        "        priorBox.addAttr('variance', boxCoderVariance)\n",
        "\n",
        "        graph_def.node.extend([priorBox])\n",
        "        priorBoxes.append(priorBox.name)\n",
        "\n",
        "    # Compare this layer's output with Postprocessor/Reshape\n",
        "    addConcatNode('PriorBox/concat', priorBoxes, 'concat/axis_flatten')\n",
        "\n",
        "    # Sigmoid for classes predictions and DetectionOutput layer\n",
        "    addReshape('ClassPredictor/concat', 'ClassPredictor/concat3d', [0, -1, num_classes + 1], graph_def)\n",
        "\n",
        "    sigmoid = NodeDef()\n",
        "    sigmoid.name = 'ClassPredictor/concat/sigmoid'\n",
        "    sigmoid.op = 'Sigmoid'\n",
        "    sigmoid.input.append('ClassPredictor/concat3d')\n",
        "    graph_def.node.extend([sigmoid])\n",
        "\n",
        "    addFlatten(sigmoid.name, sigmoid.name + '/Flatten', graph_def)\n",
        "\n",
        "    detectionOut = NodeDef()\n",
        "    detectionOut.name = 'detection_out'\n",
        "    detectionOut.op = 'DetectionOutput'\n",
        "\n",
        "    if box_predictor == 'convolutional':\n",
        "        detectionOut.input.append('BoxEncodingPredictor/concat')\n",
        "    else:\n",
        "        detectionOut.input.append('BoxPredictor/concat')\n",
        "    detectionOut.input.append(sigmoid.name + '/Flatten')\n",
        "    detectionOut.input.append('PriorBox/concat')\n",
        "\n",
        "    detectionOut.addAttr('num_classes', num_classes + 1)\n",
        "    detectionOut.addAttr('share_location', True)\n",
        "    detectionOut.addAttr('background_label_id', 0)\n",
        "\n",
        "    postProcessing = config['post_processing'][0]\n",
        "    batchNMS = postProcessing['batch_non_max_suppression'][0]\n",
        "\n",
        "    if 'iou_threshold' in batchNMS:\n",
        "        detectionOut.addAttr('nms_threshold', float(batchNMS['iou_threshold'][0]))\n",
        "    else:\n",
        "        detectionOut.addAttr('nms_threshold', 0.6)\n",
        "\n",
        "    if 'score_threshold' in batchNMS:\n",
        "        detectionOut.addAttr('confidence_threshold', float(batchNMS['score_threshold'][0]))\n",
        "    else:\n",
        "        detectionOut.addAttr('confidence_threshold', 0.01)\n",
        "\n",
        "    if 'max_detections_per_class' in batchNMS:\n",
        "        detectionOut.addAttr('top_k', int(batchNMS['max_detections_per_class'][0]))\n",
        "    else:\n",
        "        detectionOut.addAttr('top_k', 100)\n",
        "\n",
        "    if 'max_total_detections' in batchNMS:\n",
        "        detectionOut.addAttr('keep_top_k', int(batchNMS['max_total_detections'][0]))\n",
        "    else:\n",
        "        detectionOut.addAttr('keep_top_k', 100)\n",
        "\n",
        "    detectionOut.addAttr('code_type', \"CENTER_SIZE\")\n",
        "\n",
        "    graph_def.node.extend([detectionOut])\n",
        "\n",
        "    while True:\n",
        "        unconnectedNodes = getUnconnectedNodes()\n",
        "        unconnectedNodes.remove(detectionOut.name)\n",
        "        if not unconnectedNodes:\n",
        "            break\n",
        "\n",
        "        for name in unconnectedNodes:\n",
        "            for i in range(len(graph_def.node)):\n",
        "                if graph_def.node[i].name == name:\n",
        "                    del graph_def.node[i]\n",
        "                    break\n",
        "\n",
        "    # Save as text.\n",
        "    graph_def.save(outputPath)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description='Run this script to get a text graph of '\n",
        "                                                 'SSD model from TensorFlow Object Detection API. '\n",
        "                                                 'Then pass it with .pb file to cv::dnn::readNetFromTensorflow function.')\n",
        "    parser.add_argument('--input', required=True, help='Path to frozen TensorFlow graph.')\n",
        "    parser.add_argument('--output', required=True, help='Path to output text graph.')\n",
        "    parser.add_argument('--config', required=True, help='Path to a *.config file is used for training.')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    createSSDGraph('frozen_inference_graph.pb', 'pipeline.config', '')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a547247afd36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_text_graph_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSSDAnchorGenerator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'tf_text_graph_common'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcqbXpI2Q4jj"
      },
      "source": [
        "A partir d'aqui, s'utilitzara per entrenar de forma persistent un podel personal noste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7UIQWvZVCl5"
      },
      "source": [
        "# Entrenament del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOi4xf3JVgNg"
      },
      "source": [
        "Manual de la llibreria per l'utilització de la nostra AI\n",
        "\n",
        "https://detecto.readthedocs.io/en/latest/\n",
        "\n",
        "En aquest apartat aprendrém a utilitzar la llibreria detecto per a entrenar el nostre model me manera personalitzada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi8oDoctM77t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "9655159a-5f3e-44e9-dbc0-a4377e0acc40"
      },
      "source": [
        "from detecto import core, utils\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convertime els arxius XML en format CSV\n",
        "utils.xml_to_csv('train_labels/', 'train_labels.csv')\n",
        "utils.xml_to_csv('val_labels/', 'val_labels.csv')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>class</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cam_image45.jpg</td>\n",
              "      <td>960</td>\n",
              "      <td>540</td>\n",
              "      <td>jack</td>\n",
              "      <td>739</td>\n",
              "      <td>166</td>\n",
              "      <td>899</td>\n",
              "      <td>403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cam_image45.jpg</td>\n",
              "      <td>960</td>\n",
              "      <td>540</td>\n",
              "      <td>nine</td>\n",
              "      <td>562</td>\n",
              "      <td>145</td>\n",
              "      <td>733</td>\n",
              "      <td>387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cam_image45.jpg</td>\n",
              "      <td>960</td>\n",
              "      <td>540</td>\n",
              "      <td>king</td>\n",
              "      <td>365</td>\n",
              "      <td>150</td>\n",
              "      <td>550</td>\n",
              "      <td>399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cam_image45.jpg</td>\n",
              "      <td>960</td>\n",
              "      <td>540</td>\n",
              "      <td>ace</td>\n",
              "      <td>150</td>\n",
              "      <td>174</td>\n",
              "      <td>358</td>\n",
              "      <td>434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IMG_2383.JPG</td>\n",
              "      <td>378</td>\n",
              "      <td>504</td>\n",
              "      <td>ace</td>\n",
              "      <td>97</td>\n",
              "      <td>136</td>\n",
              "      <td>275</td>\n",
              "      <td>368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>IMG_2654.JPG</td>\n",
              "      <td>378</td>\n",
              "      <td>504</td>\n",
              "      <td>nine</td>\n",
              "      <td>176</td>\n",
              "      <td>135</td>\n",
              "      <td>368</td>\n",
              "      <td>383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>IMG_2675.JPG</td>\n",
              "      <td>378</td>\n",
              "      <td>504</td>\n",
              "      <td>jack</td>\n",
              "      <td>225</td>\n",
              "      <td>257</td>\n",
              "      <td>323</td>\n",
              "      <td>402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>IMG_2675.JPG</td>\n",
              "      <td>378</td>\n",
              "      <td>504</td>\n",
              "      <td>queen</td>\n",
              "      <td>166</td>\n",
              "      <td>239</td>\n",
              "      <td>231</td>\n",
              "      <td>382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>IMG_2675.JPG</td>\n",
              "      <td>378</td>\n",
              "      <td>504</td>\n",
              "      <td>nine</td>\n",
              "      <td>111</td>\n",
              "      <td>245</td>\n",
              "      <td>172</td>\n",
              "      <td>388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>IMG_2675.JPG</td>\n",
              "      <td>378</td>\n",
              "      <td>504</td>\n",
              "      <td>king</td>\n",
              "      <td>24</td>\n",
              "      <td>253</td>\n",
              "      <td>111</td>\n",
              "      <td>403</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>143 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            filename  width  height  class  xmin  ymin  xmax  ymax\n",
              "0    cam_image45.jpg    960     540   jack   739   166   899   403\n",
              "1    cam_image45.jpg    960     540   nine   562   145   733   387\n",
              "2    cam_image45.jpg    960     540   king   365   150   550   399\n",
              "3    cam_image45.jpg    960     540    ace   150   174   358   434\n",
              "4       IMG_2383.JPG    378     504    ace    97   136   275   368\n",
              "..               ...    ...     ...    ...   ...   ...   ...   ...\n",
              "138     IMG_2654.JPG    378     504   nine   176   135   368   383\n",
              "139     IMG_2675.JPG    378     504   jack   225   257   323   402\n",
              "140     IMG_2675.JPG    378     504  queen   166   239   231   382\n",
              "141     IMG_2675.JPG    378     504   nine   111   245   172   388\n",
              "142     IMG_2675.JPG    378     504   king    24   253   111   403\n",
              "\n",
              "[143 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME-4piZcn1Xy"
      },
      "source": [
        "# Definim una transformació personalitzada per aplicar al nostre dataset\n",
        "custom_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(800),\n",
        "    transforms.ColorJitter(saturation=0.3),\n",
        "    transforms.ToTensor(),\n",
        "    utils.normalize_transform(),\n",
        "])\n",
        "\n",
        "# Introduïu un fitxer CSV en lloc dels fitxers XML per obtenir una velocitat \n",
        "# d’inicialització més ràpida del conjunt de dades\n",
        "dataset = core.Dataset('train_labels.csv', 'train_images/')\n",
        "val_dataset = core.Dataset('val_labels.csv', 'val_images')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1DCX25on4Wo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        },
        "outputId": "2056cc2b-b04e-4dcd-c061-17701f2459e5"
      },
      "source": [
        "# Cream el vostre DataLoader amb opcions personalitzades\n",
        "loader = core.DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "model = core.Model(['jack', 'king', 'ten', 'nine', 'ace', 'queen'])\n",
        "losses = model.fit(loader, val_dataset, epochs=20, verbose=True)\n",
        "\n",
        "plt.plot(losses)  # Visualize loss throughout training\n",
        "plt.show()\n",
        "\n",
        "model.save('model_carta_3.pth')  # Save model to a file\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 of 20\n",
            "Loss: 0.27783702959964324\n",
            "Epoch 2 of 20\n",
            "Loss: 0.18853598325089974\n",
            "Epoch 3 of 20\n",
            "Loss: 0.17755504145280465\n",
            "Epoch 4 of 20\n",
            "Loss: 0.15929188266709135\n",
            "Epoch 5 of 20\n",
            "Loss: 0.15724835438015577\n",
            "Epoch 6 of 20\n",
            "Loss: 0.15332155463683023\n",
            "Epoch 7 of 20\n",
            "Loss: 0.1523451828560629\n",
            "Epoch 8 of 20\n",
            "Loss: 0.15255151132864136\n",
            "Epoch 9 of 20\n",
            "Loss: 0.15159549723034138\n",
            "Epoch 10 of 20\n",
            "Loss: 0.15112397359957644\n",
            "Epoch 11 of 20\n",
            "Loss: 0.15180225678152973\n",
            "Epoch 12 of 20\n",
            "Loss: 0.15276142158968883\n",
            "Epoch 13 of 20\n",
            "Loss: 0.15248272781605487\n",
            "Epoch 14 of 20\n",
            "Loss: 0.15121397404716566\n",
            "Epoch 15 of 20\n",
            "Loss: 0.1527133225300512\n",
            "Epoch 16 of 20\n",
            "Loss: 0.15239081832651907\n",
            "Epoch 17 of 20\n",
            "Loss: 0.15101675336229634\n",
            "Epoch 18 of 20\n",
            "Loss: 0.15216757755700525\n",
            "Epoch 19 of 20\n",
            "Loss: 0.15062332085587762\n",
            "Epoch 20 of 20\n",
            "Loss: 0.15242746607823807\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bloxul2nn_po",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92266cef-cfda-4baa-e040-359ede814dfc"
      },
      "source": [
        "# Directly access underlying torchvision model for even more control\n",
        "torch_model = model.get_internal_model()\n",
        "print(type(torch_model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torchvision.models.detection.faster_rcnn.FasterRCNN'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVmuQpl4tAaB"
      },
      "source": [
        "from detecto import visualize\n",
        "image = utils.read_image('val_images/cam_image4.jpg')\n",
        "\n",
        "labels, boxes, scores = model.predict(image)\n",
        "visualize.show_labeled_image(image, boxes, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou3uvtos2gVa"
      },
      "source": [
        "model.save('model_carta_2.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GymmjThqV1jb"
      },
      "source": [
        "# Proba del model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u-J2s18KWYr"
      },
      "source": [
        "# Instanciem els inports corresponents \n",
        "from detecto import core, utils, visualize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3In-7FfHTZId"
      },
      "source": [
        "model = Model.load('model_weights.pth', ['tick', 'gate'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KErlgXJSKoID"
      },
      "source": [
        "image = utils.read_image('images/IMG_2387.JPG')\n",
        "model = core.Model(['person'])\n",
        "\n",
        "labels, boxes, scores = model.predict(image)\n",
        "visualize.show_labeled_image(image, boxes, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unNBEiGaLLoM"
      },
      "source": [
        "from detecto import core, utils, visualize\n",
        "image = utils.read_image('val_images/IMG_2560.JPG')\n",
        "\n",
        "model = core.Model.load('model_carta_3.pth', ['jack', 'king', 'ten', 'nine', 'ace', 'queen'])\n",
        "\n",
        "labels, boxes, scores = model.predict(image)\n",
        "visualize.show_labeled_image(image, boxes, labels)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}